# ğŸš€ TinyGrad

> A tiny, elegant automatic differentiation engine built from scratch.

TinyGrad is a lightweight module for **automatic gradient computation**, the core mechanism behind training neural networks using **backpropagation**.

Inspired by:
- ğŸ”¬ [MicroGrad](https://github.com/karpathy/micrograd)
- ğŸ”¥ [PyTorch Autograd](https://docs.pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)

...but extended to support more features than MicroGrad, without all the complexity of PyTorch. This module supports building **complete neural networks** and has a **clear educational purpose**.

---

## âœ¨ Features

- ğŸ§  Reverse-mode automatic differentiation (backpropagation)
- ğŸ”¢ Scalar-based computational graph
- ğŸ”„ Dynamic graph construction
- ğŸ—ï¸ Build full neural networks (Pending implementation ğŸ—“ï¸)
- ğŸª¶ Lightweight and easy to read
- ğŸ“š Fully Educational

---

## ğŸ“¦ Installation

You can run the following command to install `tinygrad` in your system/virtual environment:

`pip install tinygrad@git+https://github.com/RaulBarbaRojas/TinyGrad.git`

## ğŸ§© Example Usage

Pending definition ğŸ—“ï¸